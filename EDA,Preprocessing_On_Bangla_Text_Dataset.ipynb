{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "EDA,Preprocessing On Bangla Text Dataset",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BasitHussain5/Bangala-Text-NLP-Project/blob/main/EDA%2CPreprocessing_On_Bangla_Text_Dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'bangla-stopwords:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F806606%2F1382412%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20241007%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20241007T051649Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D3327cd837c5411c4684acc807d27c348ca14838d222d7f0aa3c2e1db5b66ac755e5f4f4949344b31df421105eb4fc50aee14cf725df4c26d5ffbba8b5473fd80a0e19854e24016cf185c483ddf30e0af32f95a2fb0ac154bb24083d5e0b4d03521d33c675c15bae1960022e97a660b5ad8228910133e5f7ed320c6453c5b6fe07f50c5a146bf58950447a015d850436194cf532079af862d42acaaa71647e5809180b95a2b390309c2858038487ece393d1dceec5de8d0bca579fb479d3545c071f633b73e931126b0598e0b6e2deadb3da8698178c1733cd1e6cd84b14721a54173c683f30fa50fb0299be683be136bffb4b8495c556ae62d46212df8215aff,siyam-rupali-text:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F989926%2F1671368%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20241007%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20241007T051649Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D163c75c8f0e9f141e3e8cd9458c4ed9d35ed1f143fe241ec569f68ed6480e6b71d52dc49c2761666feac74cd03a0a7e2626290eafce06106e246fe11bee7e18cf143892990fb9e5a1432ebb0e13f34c2266e19ec889eee0ceedda6656927ccac54191699e977263cd1609839fec7d0ce07421d754956a100bde38fb7a7ed410b29c5e9ca19bf0a27c363aa6cb2324681fb0164829f1dc604e55d13dfc15cf222334749194b71293bf6b779dc70fb8cc47db2b3ad816cf4aeaa2708758f908206d5845ff38eebbfb646f2e528e614cc2ba8ae19439deab929d11fffd97f933a35fdb55ed33cb0ef3629d85a3fddfe4139b70050b4188a56d2a16742f7ccc055ce,textcategorizationbangla:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F1755595%2F2866856%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20241007%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20241007T051649Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D644187bb06193811c73fe6b679ad78369a52f440d0a687345e5c9f76a1b90baf4e32084c1d92dbd6dba70cb470d2dff06fd3dcf4187d6701f44af149b6928e90f2ccea016916c8a7ef8de3b3b0bf2c452eff6f7f3e5100754c6871ec71ab58fb39632426a03541bf90384be0174d39df4be59f478a158b29501a4cb0c07111f24496640de678741b33828be6d9c3e0fe48da84d22a7df1d7290ec079865a399a40659a9156221b14b5ffcb501b87744e3a8478268f369eb890d851119703e2aaa3e9946eec6231865ed51332accac22375e2b7593ed947efb97eef022195568d5018f6e5ac45dfedac09e898e73bde9b11e935f0327cc4f012d2c303e1b5eda0'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "4Jxz2ocBpM6O"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2021-11-29T16:42:12.731857Z",
          "iopub.execute_input": "2021-11-29T16:42:12.732205Z",
          "iopub.status.idle": "2021-11-29T16:42:12.745557Z",
          "shell.execute_reply.started": "2021-11-29T16:42:12.732168Z",
          "shell.execute_reply": "2021-11-29T16:42:12.744747Z"
        },
        "trusted": true,
        "id": "N_iYLOtkpM6T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Updating/Installing Libraries**"
      ],
      "metadata": {
        "id": "Fgl2dYrupM6V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install keras\n",
        "#!pip install --upgrade tensorflow\n",
        "#!pip install --upgrade tensorflow-gpu\n",
        "!pip install xlrd==1.2.0\n",
        "!pip install openpyxl"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-11-29T16:42:12.747503Z",
          "iopub.execute_input": "2021-11-29T16:42:12.747974Z",
          "iopub.status.idle": "2021-11-29T16:42:29.47182Z",
          "shell.execute_reply.started": "2021-11-29T16:42:12.747932Z",
          "shell.execute_reply": "2021-11-29T16:42:29.470893Z"
        },
        "trusted": true,
        "id": "DfGaPWWppM6X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Importing Libraries**"
      ],
      "metadata": {
        "id": "fXWvpKEbpM6Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from pandas import read_excel\n",
        "import numpy as np\n",
        "import re\n",
        "from re import sub\n",
        "import multiprocessing\n",
        "from unidecode import unidecode\n",
        "import os\n",
        "from time import time\n",
        "from collections import Counter\n",
        "\n",
        "from sklearn import metrics\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.python.keras.layers import LSTM,Dense,Dropout,Activation,Embedding,Flatten,Bidirectional,MaxPooling2D, Conv1D, MaxPooling1D\n",
        "from tensorflow.python.keras.models import Sequential\n",
        "\n",
        "from tensorflow.keras.optimizers import SGD,Adam\n",
        "from tensorflow.keras import regularizers\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "\n",
        "import h5py\n",
        "import csv\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "import re\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.util import ngrams"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-11-29T16:42:29.475042Z",
          "iopub.execute_input": "2021-11-29T16:42:29.475402Z",
          "iopub.status.idle": "2021-11-29T16:42:31.236269Z",
          "shell.execute_reply.started": "2021-11-29T16:42:29.475354Z",
          "shell.execute_reply": "2021-11-29T16:42:31.235636Z"
        },
        "trusted": true,
        "id": "w0y3uMtlpM6Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Loading Dataset**"
      ],
      "metadata": {
        "id": "q3sjqIvVpM6Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train=pd.read_csv(\"/kaggle/input/textcategorizationbangla/train_data_stemmer.csv\")\n",
        "df_test=pd.read_csv(\"/kaggle/input/textcategorizationbangla/test_data_stemmer.csv\")\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-11-29T16:42:31.237332Z",
          "iopub.execute_input": "2021-11-29T16:42:31.237902Z",
          "iopub.status.idle": "2021-11-29T16:43:02.753308Z",
          "shell.execute_reply.started": "2021-11-29T16:42:31.237867Z",
          "shell.execute_reply": "2021-11-29T16:43:02.752516Z"
        },
        "trusted": true,
        "id": "VVBifmXtpM6a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(df_train)\n",
        "display(df_test)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-11-29T16:43:02.756529Z",
          "iopub.execute_input": "2021-11-29T16:43:02.756846Z",
          "iopub.status.idle": "2021-11-29T16:43:02.795955Z",
          "shell.execute_reply.started": "2021-11-29T16:43:02.756804Z",
          "shell.execute_reply": "2021-11-29T16:43:02.795167Z"
        },
        "trusted": true,
        "id": "P5tm8lUypM6a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TrainSet Description**"
      ],
      "metadata": {
        "id": "2t6rKQgLpM6b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"First rows of TrainSet....\")\n",
        "df_train.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-11-29T16:43:02.797129Z",
          "iopub.execute_input": "2021-11-29T16:43:02.797348Z",
          "iopub.status.idle": "2021-11-29T16:43:02.809396Z",
          "shell.execute_reply.started": "2021-11-29T16:43:02.797321Z",
          "shell.execute_reply": "2021-11-29T16:43:02.808649Z"
        },
        "trusted": true,
        "id": "Kwz7cr6KpM6c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Description of TrainSet...\")\n",
        "df_train.describe()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-11-29T16:43:02.810697Z",
          "iopub.execute_input": "2021-11-29T16:43:02.8109Z",
          "iopub.status.idle": "2021-11-29T16:43:02.846561Z",
          "shell.execute_reply.started": "2021-11-29T16:43:02.810876Z",
          "shell.execute_reply": "2021-11-29T16:43:02.845732Z"
        },
        "trusted": true,
        "id": "YZHWhSeppM6d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Dataframe Information on TrainSet...\")\n",
        "df_train.info()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-11-29T16:43:02.847725Z",
          "iopub.execute_input": "2021-11-29T16:43:02.848338Z",
          "iopub.status.idle": "2021-11-29T16:43:02.911332Z",
          "shell.execute_reply.started": "2021-11-29T16:43:02.848302Z",
          "shell.execute_reply": "2021-11-29T16:43:02.91051Z"
        },
        "trusted": true,
        "id": "xCwKmGQSpM6d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Categories of Trainset:\")\n",
        "print(df_train.category.unique())\n",
        "category_list=df_train.category.unique()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-11-29T16:43:02.912611Z",
          "iopub.execute_input": "2021-11-29T16:43:02.914917Z",
          "iopub.status.idle": "2021-11-29T16:43:02.937334Z",
          "shell.execute_reply.started": "2021-11-29T16:43:02.914878Z",
          "shell.execute_reply": "2021-11-29T16:43:02.936782Z"
        },
        "trusted": true,
        "id": "dJSsS0zapM6e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Testset Description**"
      ],
      "metadata": {
        "id": "xpNIEXJVpM6f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"First rows of TrainSet....\")\n",
        "df_test.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-11-29T16:43:02.93838Z",
          "iopub.execute_input": "2021-11-29T16:43:02.938716Z",
          "iopub.status.idle": "2021-11-29T16:43:02.95015Z",
          "shell.execute_reply.started": "2021-11-29T16:43:02.938686Z",
          "shell.execute_reply": "2021-11-29T16:43:02.94953Z"
        },
        "trusted": true,
        "id": "0SQ3f7p8pM6g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Description of TrainSe....\")\n",
        "df_test.describe()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-11-29T16:43:02.951413Z",
          "iopub.execute_input": "2021-11-29T16:43:02.951785Z",
          "iopub.status.idle": "2021-11-29T16:43:02.97092Z",
          "shell.execute_reply.started": "2021-11-29T16:43:02.951757Z",
          "shell.execute_reply": "2021-11-29T16:43:02.970144Z"
        },
        "trusted": true,
        "id": "abYgcbwHpM6g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Information of TrainSet dataframe....\")\n",
        "df_test.info()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-11-29T16:43:02.972297Z",
          "iopub.execute_input": "2021-11-29T16:43:02.972639Z",
          "iopub.status.idle": "2021-11-29T16:43:03.03398Z",
          "shell.execute_reply.started": "2021-11-29T16:43:02.972597Z",
          "shell.execute_reply": "2021-11-29T16:43:03.033155Z"
        },
        "trusted": true,
        "id": "PDYBoiBHpM6h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Categories:\")\n",
        "print(df_train.category.unique())\n",
        "category_list=df_train.category.unique()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-11-29T16:43:03.035176Z",
          "iopub.execute_input": "2021-11-29T16:43:03.03541Z",
          "iopub.status.idle": "2021-11-29T16:43:03.060608Z",
          "shell.execute_reply.started": "2021-11-29T16:43:03.035372Z",
          "shell.execute_reply": "2021-11-29T16:43:03.059854Z"
        },
        "trusted": true,
        "id": "5fRi1IQ3pM6m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Removing Null Values**"
      ],
      "metadata": {
        "id": "hz3GAOn5pM6m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.dropna(inplace=True)\n",
        "df_test.dropna(inplace=True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-11-29T16:43:03.06372Z",
          "iopub.execute_input": "2021-11-29T16:43:03.064364Z",
          "iopub.status.idle": "2021-11-29T16:43:03.184157Z",
          "shell.execute_reply.started": "2021-11-29T16:43:03.06433Z",
          "shell.execute_reply": "2021-11-29T16:43:03.183233Z"
        },
        "trusted": true,
        "id": "vAzUk3ZBpM6n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Removing Low Length Data**"
      ],
      "metadata": {
        "id": "YcNvP9U8pM6n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#counting text length\n",
        "df_train['count'] = df_train['cleanText'].str.split().str.len()\n",
        "df_test['count'] = df_test['cleanText'].str.split().str.len()\n",
        "# Remove the text with words less than 5\n",
        "df_train= df_train.loc[df_train['count']>5]\n",
        "df_test= df_test.loc[df_test['count']>5]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-11-29T16:43:03.185434Z",
          "iopub.execute_input": "2021-11-29T16:43:03.185719Z",
          "iopub.status.idle": "2021-11-29T16:43:11.520478Z",
          "shell.execute_reply.started": "2021-11-29T16:43:03.185689Z",
          "shell.execute_reply": "2021-11-29T16:43:11.519628Z"
        },
        "trusted": true,
        "id": "tK9k5_fdpM6n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Preprocessing/Cleaning**\n",
        "Though the dataset is said to have been cleaned,but i have applied it anyway to remove Unecessary charachters,Emojis,Punctuations with the help of Regex."
      ],
      "metadata": {
        "id": "n_NFsJbepM6n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def text_to_word_list(text):\n",
        "    text = text.split()\n",
        "    return text\n",
        "\n",
        "def replace_strings(text):\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           u\"\\U00002702-\\U000027B0\"\n",
        "                           u\"\\U000024C2-\\U0001F251\"\n",
        "                           u\"\\u00C0-\\u017F\"          #latin\n",
        "                           u\"\\u2000-\\u206F\"          #generalPunctuations\n",
        "\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "    english_pattern=re.compile('[a-zA-Z0-9]+', flags=re.I)\n",
        "    #latin_pattern=re.compile('[A-Za-z\\u00C0-\\u00D6\\u00D8-\\u00f6\\u00f8-\\u00ff\\s]*',)\n",
        "\n",
        "    text=emoji_pattern.sub(r'', text)\n",
        "    text=english_pattern.sub(r'', text)\n",
        "\n",
        "    return text\n",
        "\n",
        "def remove_punctuations(my_str):\n",
        "    # define punctuation\n",
        "    punctuations = '''```\u0012\u0010\u0002\b`\u0007\b£|¢|\u0007Ñ+-*/=EROero৳০১২৩৪৫৬৭৮৯012–34567•89।!()-[]{};:'\"“\\’,<>./?@#$%^&*_~‘—॥”‰🤣⚽️✌�￰৷￰'''\n",
        "\n",
        "    no_punct = \"\"\n",
        "    for char in my_str:\n",
        "        if char not in punctuations:\n",
        "            no_punct = no_punct + char\n",
        "\n",
        "    # display the unpunctuated string\n",
        "    return no_punct\n",
        "\n",
        "\n",
        "\n",
        "def joining(text):\n",
        "    out=' '.join(text)\n",
        "    return out\n",
        "\n",
        "def preprocessing(text):\n",
        "    out=remove_punctuations(replace_strings(text))\n",
        "    return out"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-11-29T16:43:11.521792Z",
          "iopub.execute_input": "2021-11-29T16:43:11.521999Z",
          "iopub.status.idle": "2021-11-29T16:43:12.096801Z",
          "shell.execute_reply.started": "2021-11-29T16:43:11.521973Z",
          "shell.execute_reply": "2021-11-29T16:43:12.095749Z"
        },
        "trusted": true,
        "id": "YA6eRFXYpM6o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#df_train['cleanText'] = df_train.cleanText.apply(lambda x: preprocessing(str(x)))\n",
        "#df_test['cleanText'] = df_test.cleanText.apply(lambda x: preprocessing(str(x)))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-11-29T16:43:12.098309Z",
          "iopub.execute_input": "2021-11-29T16:43:12.098558Z",
          "iopub.status.idle": "2021-11-29T16:43:12.109119Z",
          "shell.execute_reply.started": "2021-11-29T16:43:12.098531Z",
          "shell.execute_reply": "2021-11-29T16:43:12.107992Z"
        },
        "trusted": true,
        "id": "e7FrB5xxpM6p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Stopwords Removal**"
      ],
      "metadata": {
        "id": "W_JFSrrrpM6p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a stopwords list containing about 700+ bangla stopwords. I have manually collected this stopwords."
      ],
      "metadata": {
        "id": "TMzpO6n3pM6p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data1 =pd.read_excel('/kaggle/input/bangla-stopwords/stopwords_bangla.xlsx')\n",
        "stop = data1['words'].tolist()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-11-29T16:43:12.110208Z",
          "iopub.execute_input": "2021-11-29T16:43:12.110421Z",
          "iopub.status.idle": "2021-11-29T16:43:12.459332Z",
          "shell.execute_reply.started": "2021-11-29T16:43:12.110397Z",
          "shell.execute_reply": "2021-11-29T16:43:12.458271Z"
        },
        "trusted": true,
        "id": "_0ezuMmFpM6z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(data1)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-11-29T16:43:12.46094Z",
          "iopub.execute_input": "2021-11-29T16:43:12.461537Z",
          "iopub.status.idle": "2021-11-29T16:43:12.476664Z",
          "shell.execute_reply.started": "2021-11-29T16:43:12.461493Z",
          "shell.execute_reply": "2021-11-29T16:43:12.475515Z"
        },
        "trusted": true,
        "id": "nxdTqrBdpM60"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def stopwordRemoval(text):\n",
        "    x=str(text)\n",
        "    l=x.split()\n",
        "\n",
        "    stm=[elem for elem in l if elem not in stop]\n",
        "\n",
        "    out=' '.join(stm)\n",
        "\n",
        "    return str(out)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-11-29T16:43:12.478713Z",
          "iopub.execute_input": "2021-11-29T16:43:12.479372Z",
          "iopub.status.idle": "2021-11-29T16:43:12.48739Z",
          "shell.execute_reply.started": "2021-11-29T16:43:12.479324Z",
          "shell.execute_reply": "2021-11-29T16:43:12.486561Z"
        },
        "trusted": true,
        "id": "5ElIxfiRpM61"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#df_train['cleanText'] = df_train.cleanText.apply(lambda x: stopwordRemoval(str(x)))\n",
        "#df_test['cleanText'] = df_test.cleanText.apply(lambda x: stopwordRemoval(str(x)))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-11-29T16:43:12.488842Z",
          "iopub.execute_input": "2021-11-29T16:43:12.489292Z",
          "iopub.status.idle": "2021-11-29T16:43:12.49859Z",
          "shell.execute_reply.started": "2021-11-29T16:43:12.48925Z",
          "shell.execute_reply": "2021-11-29T16:43:12.497834Z"
        },
        "trusted": true,
        "id": "_L8t82XrpM62"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Stemming**"
      ],
      "metadata": {
        "id": "9NU9HvNjpM63"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#make sure to turn on internet on your kernel\n",
        "#importing stemmer\n",
        "!pip install bangla-stemmer\n",
        "from bangla_stemmer.stemmer import stemmer\n",
        "## stemmer function\n",
        "def stem_text (x):\n",
        "  stmr = stemmer.BanglaStemmer()\n",
        "  words=x.split(' ')\n",
        "  stm = stmr.stem(words)\n",
        "  words=(' ').join(stm)\n",
        "  return words"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-11-29T16:43:12.500341Z",
          "iopub.execute_input": "2021-11-29T16:43:12.500876Z",
          "iopub.status.idle": "2021-11-29T16:43:21.463288Z",
          "shell.execute_reply.started": "2021-11-29T16:43:12.500836Z",
          "shell.execute_reply": "2021-11-29T16:43:21.462447Z"
        },
        "trusted": true,
        "id": "McWHGbEypM64"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#df_train['cleanText']=df_train['cleanText'].apply(stem_text)\n",
        "#df_test['cleanText']=df_test['cleanText'].apply(stem_text)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-11-29T16:43:21.465047Z",
          "iopub.execute_input": "2021-11-29T16:43:21.465288Z",
          "iopub.status.idle": "2021-11-29T16:43:21.46877Z",
          "shell.execute_reply.started": "2021-11-29T16:43:21.465259Z",
          "shell.execute_reply": "2021-11-29T16:43:21.467978Z"
        },
        "trusted": true,
        "id": "ilVNQtrgpM64"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used a Bangla Stemmer for the dataset. But through experimentation,i have seen than stemming doesn't have that much effect on our accuracy measures. So i have commented out the corresponding code."
      ],
      "metadata": {
        "id": "a55lNoTbpM65"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Category Wise Data Distribution**"
      ],
      "metadata": {
        "id": "uvPaNw_EpM65"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.countplot(df_train['category']);"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-11-29T16:43:21.469896Z",
          "iopub.execute_input": "2021-11-29T16:43:21.47013Z",
          "iopub.status.idle": "2021-11-29T16:43:21.877242Z",
          "shell.execute_reply.started": "2021-11-29T16:43:21.470104Z",
          "shell.execute_reply": "2021-11-29T16:43:21.876408Z"
        },
        "trusted": true,
        "id": "Ul6XShucpM65"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.set(rc={'figure.figsize':(11.7,8.27)})\n",
        "sns.countplot(df_test['category']);"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-11-29T16:43:21.878269Z",
          "iopub.execute_input": "2021-11-29T16:43:21.878476Z",
          "iopub.status.idle": "2021-11-29T16:43:22.843789Z",
          "shell.execute_reply.started": "2021-11-29T16:43:21.878439Z",
          "shell.execute_reply": "2021-11-29T16:43:22.842952Z"
        },
        "trusted": true,
        "id": "Y6nxMV-6pM66"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In both train set and test set,the data have no imbalance problem. We can see that every class have sufficient amount of data."
      ],
      "metadata": {
        "id": "Q6-jBCnBpM7D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Count of Texts in Each Category**"
      ],
      "metadata": {
        "id": "NOrzjRzApM7D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"IN TRAIN SET...\")\n",
        "temp1 = df_train.groupby('category').count()['cleanText'].reset_index().sort_values(by='cleanText',ascending=False)\n",
        "temp1.style.background_gradient(cmap='Purples')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-11-29T16:43:22.845047Z",
          "iopub.execute_input": "2021-11-29T16:43:22.846642Z",
          "iopub.status.idle": "2021-11-29T16:43:22.990081Z",
          "shell.execute_reply.started": "2021-11-29T16:43:22.846598Z",
          "shell.execute_reply": "2021-11-29T16:43:22.989155Z"
        },
        "trusted": true,
        "id": "tUDF0li0pM7E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"In Test Set...\")\n",
        "temp2 = df_test.groupby('category').count()['cleanText'].reset_index().sort_values(by='cleanText',ascending=False)\n",
        "temp2.style.background_gradient(cmap='Purples')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-11-29T16:43:22.991132Z",
          "iopub.execute_input": "2021-11-29T16:43:22.99209Z",
          "iopub.status.idle": "2021-11-29T16:43:23.063908Z",
          "shell.execute_reply.started": "2021-11-29T16:43:22.992053Z",
          "shell.execute_reply": "2021-11-29T16:43:23.063392Z"
        },
        "trusted": true,
        "id": "GOmp8nrupM7E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Funnel Chart of Data Distribution**"
      ],
      "metadata": {
        "id": "CASh-taUpM7E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from plotly import graph_objs as go\n",
        "print(\"On Train Set....\")\n",
        "fig = go.Figure(go.Funnelarea(\n",
        "    text =temp1.category,\n",
        "    values = temp1.cleanText,\n",
        "    title = {\"position\": \"top center\", \"text\": \"Funnel-Chart of Category Distribution on Train Set\"}\n",
        "    ))\n",
        "fig.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-11-29T16:43:23.065057Z",
          "iopub.execute_input": "2021-11-29T16:43:23.065463Z",
          "iopub.status.idle": "2021-11-29T16:43:23.205144Z",
          "shell.execute_reply.started": "2021-11-29T16:43:23.06541Z",
          "shell.execute_reply": "2021-11-29T16:43:23.204405Z"
        },
        "trusted": true,
        "id": "tpISD3iPpM7E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"On Test Set...\")\n",
        "fig = go.Figure(go.Funnelarea(\n",
        "    text =temp2.category,\n",
        "    values = temp2.cleanText,\n",
        "    title = {\"position\": \"top center\", \"text\": \"Funnel-Chart of Category Distribution on Test Set\"}\n",
        "    ))\n",
        "fig.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-11-29T16:43:23.206293Z",
          "iopub.execute_input": "2021-11-29T16:43:23.206543Z",
          "iopub.status.idle": "2021-11-29T16:43:23.219197Z",
          "shell.execute_reply.started": "2021-11-29T16:43:23.206515Z",
          "shell.execute_reply": "2021-11-29T16:43:23.218284Z"
        },
        "trusted": true,
        "id": "EPdJfvT8pM7F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Count Based Top Words in Each Category on Train Set**"
      ],
      "metadata": {
        "id": "X8zV5-lLpM7F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "category_list=category_list.tolist()\n",
        "for i in category_list:\n",
        "    temp=df_train.loc[df_train['category'] == str(i)]\n",
        "    #display(temp)\n",
        "    temp['temp_list'] = temp['cleanText'].apply(lambda x:str(x).split())\n",
        "    top = Counter([item for sublist in temp['temp_list'] for item in sublist])\n",
        "    temp = pd.DataFrame(top.most_common(20))\n",
        "    temp.columns = ['Common_words','count']\n",
        "    temp.style.background_gradient(cmap='Blues')\n",
        "    temp = temp.style.set_caption('Top 20 Words In '+ str(i)+\" Category\")\n",
        "    display(temp)\n",
        ""
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-11-29T16:43:23.220501Z",
          "iopub.execute_input": "2021-11-29T16:43:23.221151Z",
          "iopub.status.idle": "2021-11-29T16:43:31.200934Z",
          "shell.execute_reply.started": "2021-11-29T16:43:23.221115Z",
          "shell.execute_reply": "2021-11-29T16:43:31.200079Z"
        },
        "trusted": true,
        "id": "PjJXq5hdpM7F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Top 20 Words in TrainSet Based on Count**"
      ],
      "metadata": {
        "id": "nsUZ-GDUpM7F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df_train['temp_list'] = df_train['cleanText'].apply(lambda x:str(x).split())\n",
        "top = Counter([item for sublist in df_train['temp_list'] for item in sublist])\n",
        "temp = pd.DataFrame(top.most_common(20))\n",
        "temp.columns = ['Common_words','count']\n",
        "temp.style.background_gradient(cmap='Blues')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-11-29T16:43:31.202222Z",
          "iopub.execute_input": "2021-11-29T16:43:31.202432Z",
          "iopub.status.idle": "2021-11-29T16:43:40.025505Z",
          "shell.execute_reply.started": "2021-11-29T16:43:31.202405Z",
          "shell.execute_reply": "2021-11-29T16:43:40.024886Z"
        },
        "trusted": true,
        "id": "dm_KrXEopM7F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Top 20 Words in TestSet Based on Count**"
      ],
      "metadata": {
        "id": "q4CjD1u4pM7H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df_test['temp_list'] = df_test['cleanText'].apply(lambda x:str(x).split())\n",
        "top = Counter([item for sublist in df_test['temp_list'] for item in sublist])\n",
        "temp = pd.DataFrame(top.most_common(20))\n",
        "temp.columns = ['Common_words','count']\n",
        "temp.style.background_gradient(cmap='Blues')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-11-29T16:43:40.026569Z",
          "iopub.execute_input": "2021-11-29T16:43:40.026899Z",
          "iopub.status.idle": "2021-11-29T16:43:49.874795Z",
          "shell.execute_reply.started": "2021-11-29T16:43:40.026871Z",
          "shell.execute_reply": "2021-11-29T16:43:49.874158Z"
        },
        "trusted": true,
        "id": "L5Gr4cqnpM7I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Histogram Analysis Based on Text Length**"
      ],
      "metadata": {
        "id": "4H9d2CrhpM7K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig = go.Figure(layout=dict(title=dict(text=\"Text Length Histogram of Trainset\")))\n",
        "fig.add_trace(go.Histogram(x=df_train['count']))\n",
        "fig.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-11-29T16:43:49.87574Z",
          "iopub.execute_input": "2021-11-29T16:43:49.876274Z",
          "iopub.status.idle": "2021-11-29T16:43:49.906313Z",
          "shell.execute_reply.started": "2021-11-29T16:43:49.876239Z",
          "shell.execute_reply": "2021-11-29T16:43:49.905761Z"
        },
        "trusted": true,
        "id": "AauJqo7apM7L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = go.Figure(layout=dict(title=dict(text=\"Text Length Histogram of Testset\")))\n",
        "fig.add_trace(go.Histogram(x=df_test['count']))\n",
        "fig.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-11-29T16:43:49.907366Z",
          "iopub.execute_input": "2021-11-29T16:43:49.908172Z",
          "iopub.status.idle": "2021-11-29T16:43:49.924076Z",
          "shell.execute_reply.started": "2021-11-29T16:43:49.908136Z",
          "shell.execute_reply": "2021-11-29T16:43:49.923267Z"
        },
        "trusted": true,
        "id": "44mj_rSBpM7M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the histogram,we can say that abobut 97% of the text lenght are between 1-600. This information is valuable for model creation and text lenght selection."
      ],
      "metadata": {
        "id": "L2dBFITTpM7M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **N gram Analysis**"
      ],
      "metadata": {
        "id": "hLY4_hLwpM7N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train=pd.read_csv(\"/kaggle/input/textcategorizationbangla/train_data_stemmer.csv\")\n",
        "df_test=pd.read_csv(\"/kaggle/input/textcategorizationbangla/test_data_stemmer.csv\")\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-11-29T16:43:49.925709Z",
          "iopub.execute_input": "2021-11-29T16:43:49.925936Z",
          "iopub.status.idle": "2021-11-29T16:44:13.122197Z",
          "shell.execute_reply.started": "2021-11-29T16:43:49.925908Z",
          "shell.execute_reply": "2021-11-29T16:44:13.121324Z"
        },
        "trusted": true,
        "id": "iIbSlCt2pM7N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to create top 20 n-grams\n",
        "def get_ngrams(data,n):\n",
        "    all_words = []\n",
        "    for i in range(len(data)):\n",
        "        temp = data[\"cleanText\"][i].split()\n",
        "        for word in temp:\n",
        "            all_words.append(word)\n",
        "\n",
        "    tokenized = all_words\n",
        "    esBigrams = ngrams(tokenized, n)\n",
        "\n",
        "    esBigram_wordlist = nltk.FreqDist(esBigrams)\n",
        "    top20 = esBigram_wordlist.most_common(20)\n",
        "    top20 = dict(top20)\n",
        "    df_ngrams = pd.DataFrame(sorted(top20.items(), key=lambda x: x[1])[::-1])\n",
        "    df_ngrams.columns = ['Ngram','count']\n",
        "    return df_ngrams\n",
        "\n",
        "\n",
        "# function to visualize the top 20 n-grams\n",
        "def show(train):\n",
        "    display(train.head(20))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-11-29T16:44:13.123593Z",
          "iopub.execute_input": "2021-11-29T16:44:13.123836Z",
          "iopub.status.idle": "2021-11-29T16:44:13.131023Z",
          "shell.execute_reply.started": "2021-11-29T16:44:13.123807Z",
          "shell.execute_reply": "2021-11-29T16:44:13.130185Z"
        },
        "trusted": true,
        "id": "niCMFNEopM7O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Top 20 Unigram Count Based On Category**"
      ],
      "metadata": {
        "id": "2NNEs61jpM7O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in category_list:\n",
        "    temp=df_train.loc[df_train['category'] == str(i)]\n",
        "    #display(temp)\n",
        "    temp['temp_list'] = temp['cleanText'].apply(lambda x:str(x).split())\n",
        "    temp.reset_index(drop=True, inplace=True)\n",
        "    train_unigrams = get_ngrams(temp,1)\n",
        "    print(\"\\t\\t\\t====== Unigrams of \"+str(i)+\"======\")\n",
        "    show(train_unigrams)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-11-29T16:44:13.132186Z",
          "iopub.execute_input": "2021-11-29T16:44:13.132391Z",
          "iopub.status.idle": "2021-11-29T16:44:48.281992Z",
          "shell.execute_reply.started": "2021-11-29T16:44:13.132366Z",
          "shell.execute_reply": "2021-11-29T16:44:48.280919Z"
        },
        "trusted": true,
        "id": "N6lwd84fpM7O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Top 20 Bigram Count Based On Category**"
      ],
      "metadata": {
        "id": "Q7LzIhM0pM7P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in category_list:\n",
        "    temp=df_train.loc[df_train['category'] == str(i)]\n",
        "    #display(temp)\n",
        "    temp['temp_list'] = temp['cleanText'].apply(lambda x:str(x).split())\n",
        "    temp.reset_index(drop=True, inplace=True)\n",
        "    train_bigrams = get_ngrams(temp,2)\n",
        "    print(\"\\t\\t\\t====== Bigrams of \"+str(i)+\" ======\")\n",
        "    show(train_bigrams)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-11-29T16:44:48.284565Z",
          "iopub.execute_input": "2021-11-29T16:44:48.284939Z",
          "iopub.status.idle": "2021-11-29T16:45:31.268906Z",
          "shell.execute_reply.started": "2021-11-29T16:44:48.284886Z",
          "shell.execute_reply": "2021-11-29T16:45:31.268079Z"
        },
        "trusted": true,
        "id": "wCiTsZZRpM7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Top 20 Trigram Count Based On Category**"
      ],
      "metadata": {
        "id": "QudIoGjWpM7Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in category_list:\n",
        "    temp=df_train.loc[df_train['category'] == str(i)]\n",
        "    #display(temp)\n",
        "    temp['temp_list'] = temp['cleanText'].apply(lambda x:str(x).split())\n",
        "    temp.reset_index(drop=True, inplace=True)\n",
        "    train_trigrams = get_ngrams(temp,3)\n",
        "    print(\"\\t\\t\\t====== Trigrams of \"+str(i)+\" ======\")\n",
        "    show(train_trigrams)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-11-29T16:45:31.270327Z",
          "iopub.execute_input": "2021-11-29T16:45:31.270583Z",
          "iopub.status.idle": "2021-11-29T16:46:14.440503Z",
          "shell.execute_reply.started": "2021-11-29T16:45:31.270534Z",
          "shell.execute_reply": "2021-11-29T16:46:14.439624Z"
        },
        "trusted": true,
        "id": "QQ4lo2VipM7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Word Cloud Based On Category**"
      ],
      "metadata": {
        "id": "gK59xR9zpM7R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing wordcloud for plotting word clouds and textwrap for wrapping longer text\n",
        "from wordcloud import WordCloud\n",
        "from textwrap import wrap\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import font_manager\n",
        "\n",
        "# Function for generating word clouds\n",
        "def generate_wordcloud(data,title):\n",
        "  data = [tuple(x) for x in data.values]\n",
        "  wc = WordCloud(font_path=\"/kaggle/input/siyam-rupali-text/Siyamrupali.ttf\",width=1080, height=720, max_words=150,colormap=\"Dark2\").generate_from_frequencies(dict(data))\n",
        "  plt.figure(figsize=(10,8))\n",
        "  plt.imshow(wc, interpolation='bilinear')\n",
        "  plt.axis(\"off\")\n",
        "  plt.title('\\n'.join(wrap(\"Word Cloud of \"+title,60)),fontsize=13)\n",
        "  plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-11-29T16:46:14.442001Z",
          "iopub.execute_input": "2021-11-29T16:46:14.442268Z",
          "iopub.status.idle": "2021-11-29T16:46:14.491953Z",
          "shell.execute_reply.started": "2021-11-29T16:46:14.442229Z",
          "shell.execute_reply": "2021-11-29T16:46:14.491317Z"
        },
        "trusted": true,
        "id": "5cZfADEfpM7R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for i in category_list:\n",
        "    temp=df_train.loc[df_train['category'] == str(i)]\n",
        "    #display(temp)\n",
        "    temp['temp_list'] = temp['cleanText'].apply(lambda x:str(x).split())\n",
        "    top = Counter([item for sublist in temp['temp_list'] for item in sublist])\n",
        "    temp = pd.DataFrame(top.most_common(500000))\n",
        "    temp.columns = ['Common_words','count']\n",
        "    generate_wordcloud(temp,str(i))\n",
        ""
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-11-29T16:46:14.496939Z",
          "iopub.execute_input": "2021-11-29T16:46:14.49765Z",
          "iopub.status.idle": "2021-11-29T16:46:46.680602Z",
          "shell.execute_reply.started": "2021-11-29T16:46:14.497613Z",
          "shell.execute_reply": "2021-11-29T16:46:46.679672Z"
        },
        "trusted": true,
        "id": "mB5jWTLlpM7R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion Of EDA on banglaMCT7 Dataset**\n",
        "* **Both TrainSet and TrainSet are similar in size.**\n",
        "* **TraintSet and TestSet top word counts are similar,so we can say that TrainSet and TestSet are originated from same distribution of Data.**\n",
        "* **The dataset don't have any data imbalance problem,every category have sufficient amount of data.**\n",
        "* **Histogram analaysis shows that most of the text are of lenght 1-600.**\n",
        "* **Category wise Ngram shows that the category wised created ngrams are relavent to that category.**\n",
        "* **Category wise Word Cloud also shows result relavent to the category**"
      ],
      "metadata": {
        "id": "0tGWeX8IpM7R"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-JaslhhppM7R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yuT4GN1JpM7S"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}